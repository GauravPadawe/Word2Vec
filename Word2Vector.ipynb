{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vector using Gensim\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "#### 1. **Information**\n",
    "    - Details\n",
    "    - Objective\n",
    "\n",
    "#### 2. **Loading Dataset**\n",
    "    - Importing packages\n",
    "    - Reading Data\n",
    "    - Shape of data\n",
    "    - Dtype\n",
    "\n",
    "#### 3. **Text Cleansing**\n",
    "\n",
    "#### 4. **Modeling**\n",
    "\n",
    "#### 5. **Saving and Loading**\n",
    "\n",
    "#### 6. **Conclusion**\n",
    "\n",
    "#### 7. **What's next ?**\n",
    "\n",
    "#### 8. **References**<br>\n",
    "\n",
    "### Source :\n",
    "- https://www.kaggle.com/c/word2vec-nlp-tutorial/data\n",
    "\n",
    "### Details :\n",
    "\n",
    "- The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
    "\n",
    "- labeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.  \n",
    "\n",
    "\n",
    "- testData - The test set. The tab-delimited file has a header row followed by 25,000 rows containing an id and text for each review. Your task is to predict the sentiment for each one. \n",
    "\n",
    "\n",
    "- unlabeledTrainData - An extra training set with no labels. The tab-delimited file has a header row followed by 50,000 rows containing an id and text for each review. \n",
    "\n",
    "\n",
    "- sampleSubmission - A comma-delimited sample submission file in the correct format.\n",
    "\n",
    "\n",
    "- **Data fields :**\n",
    "    - id - Unique ID of each review\n",
    "    - sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "    - review - Text of the review\n",
    "\n",
    "\n",
    "### Objective :\n",
    "\n",
    "- The goal is to build Word2Vec Word Embedding model with the help of Gensim for NLP Tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bngF8jJuCepw"
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "\n",
    "import gensim\n",
    "#import warnings\n",
    "import re, string\n",
    "import pandas as pd\n",
    "#warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are loading Gensim here which is fairly straightforward library / module to build Word2Vec models.\n",
    "\n",
    "\n",
    "- We'll see it in more details as we move along, In case the gensim isn't installed use this command **\"!pip install gensim\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "oWLFPD-2Cep7",
    "outputId": "0ee332c7-1b4b-4b2d-973e-d2b4e0c5adf7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"45057_0\"</td>\n",
       "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"15561_0\"</td>\n",
       "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7161_0\"</td>\n",
       "      <td>\"I went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"43971_0\"</td>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                             review\n",
       "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
       "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
       "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
       "3   \"7161_0\"  \"I went to see this film with a great deal of ...\n",
       "4  \"43971_0\"  \"Yes, I agree with everyone on this site this ..."
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading dataset\n",
    "\n",
    "df = pd.read_csv('unlabeledTrainData.tsv', header=0, delimiter='\\t', quoting=3)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "sbj8j_8zHuPW",
    "outputId": "210180f7-df72-49e2-be1f-e26b2c122f8f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape od Dataset : (50000, 2)\n"
     ]
    }
   ],
   "source": [
    "#shape of dataset\n",
    "print ('Shape od Dataset :' ,df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "Oog0l49YCep_",
    "outputId": "70c913f8-0a82-4cbf-a4f7-0377d5ab434a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\\\\"Hey, let\\'s pool our money together and make a really bad movie!\\\\\" Or something like that. What ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc. All corners were cut, except the one that would have prevented this film\\'s release. Life\\'s like that.\"'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets review 1st corpus\n",
    "df['review'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can observe that our corpus may contain HTML tags, special characters, etc.\n",
    "\n",
    "\n",
    "- We need to clean the corpus by eliminating these tags and characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "2MlJTQKyJKmY",
    "outputId": "2cade9b9-512e-4874-8271-14a011e970ab",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        object\n",
       "review    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking dtypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U19bWHXYH3ZE"
   },
   "outputs": [],
   "source": [
    "#importing beautiful soup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#empty list\n",
    "clean = [ ]\n",
    "\n",
    "#for loop to clean dataset while removing html tags, special characters\n",
    "for doc in df['review']:\n",
    "    x = doc.lower()                     #lowerthe case\n",
    "    x = BeautifulSoup(x, 'lxml').text   #html tag removal\n",
    "    x = re.sub('[^A-Za-z0-9]+', ' ', x) #replacing it by space to seperate words\n",
    "    clean.append(x)\n",
    "\n",
    "#assigning clean list to new attribute\n",
    "df['clean'] = clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above we're using beautiful soup library to eliminate HTML tags. Alternate way to this is we can use Regex (Regular Expression) to eliminate tags.\n",
    "\n",
    "\n",
    "- Finally, we are appending the clean set to list and forming a new attribute as clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "GdjduJtdNEyp",
    "outputId": "84f5ea06-c56f-4638-b2cb-dd3a609fefe9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>review</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"9999_0\"</td>\n",
       "      <td>\"Watching Time Chasers, it obvious that it was...</td>\n",
       "      <td>watching time chasers it obvious that it was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"45057_0\"</td>\n",
       "      <td>\"I saw this film about 20 years ago and rememb...</td>\n",
       "      <td>i saw this film about 20 years ago and rememb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"15561_0\"</td>\n",
       "      <td>\"Minor Spoilers&lt;br /&gt;&lt;br /&gt;In New York, Joan B...</td>\n",
       "      <td>minor spoilersin new york joan barnard elvire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"7161_0\"</td>\n",
       "      <td>\"I went to see this film with a great deal of ...</td>\n",
       "      <td>i went to see this film with a great deal of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"43971_0\"</td>\n",
       "      <td>\"Yes, I agree with everyone on this site this ...</td>\n",
       "      <td>yes i agree with everyone on this site this m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  ...                                              clean\n",
       "0   \"9999_0\"  ...   watching time chasers it obvious that it was ...\n",
       "1  \"45057_0\"  ...   i saw this film about 20 years ago and rememb...\n",
       "2  \"15561_0\"  ...   minor spoilersin new york joan barnard elvire...\n",
       "3   \"7161_0\"  ...   i went to see this film with a great deal of ...\n",
       "4  \"43971_0\"  ...   yes i agree with everyone on this site this m...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#viewing data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above we can observe that we had succesfully cleaned the data set.\n",
    "\n",
    "\n",
    "- Now, its ready to pass into Word2Vec but lets learn about Word2Vec Word Embeddings :\n",
    "\n",
    "\n",
    "    - Where and Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "gyjFr49aCeqB",
    "outputId": "333da850-6740-4539-de6e-8bd8935ffa33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Doc : ['', 'watching', 'time', 'chasers', 'it', 'obvious', 'that', 'it', 'was', 'made', 'by', 'a', 'bunch', 'of', 'friends', 'maybe', 'they', 'were', 'sitting', 'around', 'one', 'day', 'in', 'film', 'school', 'and', 'said', 'hey', 'let', 's', 'pool', 'our', 'money', 'together', 'and', 'make', 'a', 'really', 'bad', 'movie', 'or', 'something', 'like', 'that', 'what', 'ever', 'they', 'said', 'they', 'still', 'ended', 'up', 'making', 'a', 'really', 'bad', 'movie', 'dull', 'story', 'bad', 'script', 'lame', 'acting', 'poor', 'cinematography', 'bottom', 'of', 'the', 'barrel', 'stock', 'music', 'etc', 'all', 'corners', 'were', 'cut', 'except', 'the', 'one', 'that', 'would', 'have', 'prevented', 'this', 'film', 's', 'release', 'life', 's', 'like', 'that', '']\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the corpus\n",
    "\n",
    "docs = [ ]\n",
    "\n",
    "for doc in df['clean']:\n",
    "    docs.append(doc.split(' '))\n",
    "\n",
    "print ('First Doc :', docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above we had Tokenized the data where we're actually splitting the words on individual level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "\n",
    "- Word2Vec is a model to form / create word Embeddings. Its a modern way of representing Word , wherein each word is represented by a vector (Array of numbers based on Embedding Size). Vectors are nothing but the weights of neurons, so if we set neurons of size 100 then we will have 100 weights and those weights are our Word Embeddings or simply Dense vector.\n",
    "\n",
    "\n",
    "- Input word must be a One hot encoded. For example :\n",
    "\n",
    "\n",
    "![alt_text](https://miro.medium.com/max/714/1*UOjWvDziH86T2MmiDpp98Q.png)\n",
    "\n",
    "\n",
    "- **Why Word2Vec ? :** Word2Vec finds relation (Semantic or Syntactic) between the words which was not possible by our Tradional TF-IDF or Frequency based approach. When we train the model, each one hot encoded word gets a point in a dimensional space where it learns and groups the words with similar meaning. \n",
    "\n",
    "\n",
    "- The neural network incorporated here is a Shallow.\n",
    "\n",
    "\n",
    "- One thing to note here is that we need large textual data to pass into Word2Vec model in order to figure out relation within words or generate meaningful results.\n",
    "\n",
    "\n",
    "- In general the Word2Vec is based on Window Method, where we have to assign a Window size. \n",
    "\n",
    "\n",
    "![alt_text](https://1.bp.blogspot.com/-nZFc7P6o3Yc/XQo2cYPM_ZI/AAAAAAAABxM/XBqYSa06oyQ_sxQzPcgnUxb5msRwDrJrQCLcBGAs/s1600/image001.png)\n",
    "\n",
    "\n",
    "- In above visual representation , Window size is set to 1. So, 1 word from both the sides of target are being considered. Similarly, in each iteration, window will slides by single stride and our neighbors will keep changing. \n",
    "\n",
    "\n",
    "- There are 2 types of Algorithms : CBOW and Skip-gram.\n",
    "\n",
    "#### (a) Continuous Bag of Words (CBOW) :\n",
    "\n",
    "- In here neighboring words are provided as Input to predict the Target. In other words, A context is Provided as input to predict the Target. For example :\n",
    "\n",
    "\n",
    "- Let us look at this in visual representation :\n",
    "\n",
    "\n",
    "![alt_text](https://www.researchgate.net/publication/283531484/figure/fig2/AS:667840583577604@1536237011465/Continuous-Bag-of-Words-model.png)\n",
    "\n",
    "\n",
    "- We can observe that **W(t)** is being predicted given that the context / neighboring words are feeded as input. P-value is estimated for target word based on the input that is feeded to the network.\n",
    "\n",
    "\n",
    "- We've a shallow neural network here. Size of each word embedding or individual dense vector is depended on the number of neurons we've at disposal.\n",
    "\n",
    "\n",
    "- These vectors are weights learnt by each neuron. \n",
    "\n",
    "\n",
    "- Regardless of how big or small the word is, it will be represented by size of embedding we set.\n",
    "\n",
    "\n",
    "- **Advantages :**\n",
    "\n",
    "    - CBOW is faster and represents frequent words more better.\n",
    "    - When it comes to memory utilization, CBOW tends to consume Low memory.\n",
    "    \n",
    "    \n",
    "- **Disadvantages :**\n",
    "    \n",
    "    - Fails to Represent Infrequent words.\n",
    "    - Needs huge textual data. (***Note : Can't say it is disadvantage since it consumes low memory but thought its worth mentioning***)\n",
    "\n",
    "\n",
    "#### (b) Skip-gram :\n",
    "\n",
    "\n",
    "- Skip-gram is opposite / inverse of CBOW, wherein a target word is provided as output in order to predict the Contextual / Neighboring words.\n",
    "\n",
    "\n",
    "- Let us look at this in visual representation :\n",
    "\n",
    "\n",
    "![alt_text](https://sakhawathsumit.github.io/sumit.log/assets/images/posts/2018-05-26-the-intuition-behind-word-embeddings-and-details-on-word2vec-skip-gram-model/skip-gram.png)\n",
    "\n",
    "\n",
    "- Here, a Target word is feeded to our shallow neural network, weights are learnt from hidden layer. One word is picked at random from the neighboring words (which are based on our Window Size). Furthermore, P-value is being calculated for context words being close to the input word we feed to the network.\n",
    "\n",
    "\n",
    "- **Advantages :**\n",
    "\n",
    "    - Skip-gram works well with smaller datasets.\n",
    "    - Its able to represent rare words well.\n",
    "    \n",
    "    \n",
    "- **Disadvantages :**\n",
    "    \n",
    "    - Slow on training if dataset is huge.\n",
    "    - And is not memory efficient.\n",
    "\n",
    "\n",
    "### But there is a PROBLEM !!!!\n",
    "\n",
    "- Both techniques are based on probability estimation.\n",
    "\n",
    "\n",
    "- Let us consider a example here, we've a corpus of 1000 words. We need to predict either a target word (CBOW) or context (Skip-gram). Now, we specify a fixed Window size for sliding through the corpus. Let's say our Window size is 1 , i.e, In case of CBOW the Input will be 2 Words (Words on both sides of target) while in case of Skip-gram input will be a single word in order to predict context (2 words).\n",
    "\n",
    "\n",
    "- In this case the probabilities will be estimated for words within a certain window and those probabilites will be higher than that for those words which are far from neighborhood. \n",
    "\n",
    "\n",
    "- Computing probabilities for huge corpus (e.g.: Millions of words) would be computationally expensive since algorithm will estimate probabilities for all the words at each iteration. In this case, The model may give irrelevant results too. And so to overcome this issue **Negative Sampling** was proposed.\n",
    "\n",
    "\n",
    "- In **Negative Sampling** the probabilities are estimated for word within our fixed Window size, just like before but only few random words are chosen out of fixed window. By this the load on p-value estimation is low compared to what it was before. Training is faster and results are more satisfactory.\n",
    "\n",
    "    \n",
    "#### CBOW or Skip-gram - which to use when ?\n",
    "\n",
    "- It depends on nature of problem.\n",
    "\n",
    "\n",
    "- Both have their own set of benefits.\n",
    "\n",
    "\n",
    "#### Gensim :\n",
    "\n",
    "- Gensim is fairly easy to use module which inherits CBOW and Skip-gram.\n",
    "\n",
    "\n",
    "- We can install it by using **!pip install gensim**.\n",
    "\n",
    "\n",
    "- Alternate way to implement Word2Vec is to build it from scratch which is quite complex.\n",
    "\n",
    "\n",
    "- Read more about Gensim : https://radimrehurek.com/gensim/index.html\n",
    "\n",
    "\n",
    "- **FYI, Gensim was developed and is maintained by the NLP researcher Radim Řehůřek and his company RaRe Technologies.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Modeling\n",
    "\n",
    "\n",
    "- Further we'll look how to implement Word2Vec to oull out Word Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbaxRIfLCeqE",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Word2vec implementation\n",
    "model = gensim.models.Word2Vec(docs, # document (list of list)\n",
    "                              min_count=10, #Ignore those words with total frequency less than 10\n",
    "                              workers=4, #Number of CPU\n",
    "                              size=50, #embedding size\n",
    "                              window=5, #Maximum distance between current and predicted word\n",
    "                              iter = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are few parameters which one could play with :\n",
    "\n",
    "- **sentences :** The sentences accepts a list of lists of tokens. (Better to have large document list in case of CBOW)\n",
    "\n",
    "\n",
    "- **size :** Number of Neurons to incorporate in hidden layer or size of Word Embeddings. By default its set to 100.\n",
    "\n",
    "\n",
    "- **window :** Window Size or Number of words to consider around target. If size = 1 then 1 word from both sides will be considered. By default 5 is fixed Window Size.\n",
    "\n",
    "\n",
    "- **min_count :** Default value is 5. Words which are infrequent based on minimum count mentioned will be ignored.\n",
    "\n",
    "\n",
    "- **workers :** Number of CPU Threads to use at once for faster training.\n",
    "\n",
    "\n",
    "- **sg :** Either 0 or 1. Default is 0 or CBOW. One must explicitly define Skip-gram by passing 1.\n",
    "\n",
    "\n",
    "- **iter :** Number of Iterations or Epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "tT9qoQVcZHsc",
    "outputId": "d008d26a-dfac-4e67-e6e8-54005c69834c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28656"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocab size\n",
    "len(model.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We've 28656 words in our vocabulary for usage after eliminating words which appeared less than 10.\n",
    "\n",
    "\n",
    "- And each word will be represented by 50 numbers / weights (which is our Embedding size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "V3_BVApFCeqH",
    "outputId": "beff177b-55d0-404d-e38f-09eb6c851fcf"
   },
   "outputs": [],
   "source": [
    "#model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yWAK_BPCeqK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#uncomment to view vocabulary\n",
    "#model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "colab_type": "code",
    "id": "ujR0r0wuCeqN",
    "outputId": "a98d8c4d-eff9-40c6-9541-d57948b27e0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.7726305 , -1.0800452 , -1.788979  ,  2.340867  , -0.32861072,\n",
       "        0.0651653 ,  1.6166486 , -3.2617207 , -3.6233435 , -3.32576   ,\n",
       "       -1.7012835 ,  1.0813012 , -0.24166667, -1.1136819 ,  1.7357157 ,\n",
       "       -2.852496  ,  0.2456542 ,  0.9012077 , -0.8035166 ,  1.7389616 ,\n",
       "        1.5673314 ,  2.0869598 ,  3.3215692 ,  0.8369672 ,  0.07051245,\n",
       "        2.9767258 , -0.92073804,  0.6535899 ,  2.716228  ,  2.5288267 ,\n",
       "        0.18343763,  1.5990931 , -2.1080818 ,  1.5348029 ,  0.19268313,\n",
       "       -1.7983583 , -0.22839952, -0.22228098,  4.939321  ,  2.071981  ,\n",
       "       -0.2585357 , -1.6617067 ,  1.3812392 , -3.7641723 ,  1.650655  ,\n",
       "       -1.4870547 ,  2.4975944 ,  2.2064195 , -2.383971  , -1.3767233 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets look at vector of great\n",
    "model.wv['great']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above we can observe that \"great\" word is represented by 50 Weights / Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "colab_type": "code",
    "id": "LfmabaOyCeqQ",
    "outputId": "9a3b957b-d4e0-42a1-d2c1-94846916165f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('stupid', 0.899315595626831),\n",
       " ('lame', 0.8604525923728943),\n",
       " ('silly', 0.8508281111717224),\n",
       " ('generic', 0.7499771118164062),\n",
       " ('pathetic', 0.7478170394897461),\n",
       " ('retarded', 0.7461548447608948),\n",
       " ('corny', 0.7456734776496887),\n",
       " ('cheesy', 0.7454365491867065),\n",
       " ('ridiculous', 0.7351659536361694),\n",
       " ('bad', 0.7300432920455933)]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find similar words to the given word\n",
    "model.wv.most_similar('dumb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One beautiful thing about Gensim is that we have such methods which can give us similar words aligned by highest probability.\n",
    "\n",
    "\n",
    "- Cosine Similarity is computed to between the words to find most similar words.\n",
    "\n",
    "\n",
    "- As we passed \"dumb\" we can observe that how most similar words are aligned by p-value.\n",
    "\n",
    "\n",
    "- Given \"dumb\" , most similar words are stupid, lame, silly, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "6pDEADM7CeqT",
    "outputId": "f0e35faf-e53e-45aa-9f12-a66fdafb4fb9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'trust'"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words which doesn't match\n",
    "model.wv.doesnt_match('house rent trust apartment'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which document from the given list doesn’t go with the others from the training set ?\n",
    "\n",
    "\n",
    "- We passed \"house\", \"rent\", \"trust\" and \"apartment\" and result is \"trust\" , being not similar to other words and its satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "K5z3ffVqCeqd",
    "outputId": "3a337691-eb52-4613-cbf1-766d099e8fc5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('girl', 0.7452774047851562)"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#arithmetic operations\n",
    "model.most_similar(positive=['woman','man'], negative=['king'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also perform basic arithmetic problems here.\n",
    "\n",
    "\n",
    "- If Woman + Man = King - ?\n",
    "\n",
    "\n",
    "- Output is \"Girl\" having p-value 0.74 , which is acceptable, If we would have larger corpora then we may get output as \"Queen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "2RXRrXqFbW4x",
    "outputId": "3bcc15f5-32d2-4de5-fcc4-4e835d3205e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.0976436 , -1.0300254 ,  1.0157741 , ...,  2.0884573 ,\n",
       "         1.2929492 , -0.50073916],\n",
       "       [-0.10799529,  1.1408263 ,  0.23857029, ..., -0.6748595 ,\n",
       "         1.4341863 , -0.03276591],\n",
       "       [-0.67246455,  1.097886  ,  0.97168344, ..., -0.7108262 ,\n",
       "        -0.20907082, -0.37925354],\n",
       "       ...,\n",
       "       [ 0.44054744,  0.13000782,  0.04757666, ...,  0.09963967,\n",
       "         0.2213286 , -0.13963003],\n",
       "       [ 0.42221656,  0.37493396,  0.28681585, ..., -0.1619114 ,\n",
       "         0.2634282 ,  0.0619109 ],\n",
       "       [ 0.43446574,  0.11213642,  0.05965824, ..., -0.02202358,\n",
       "         0.1439297 , -0.01608269]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word embeddings\n",
    "model.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above is our array having 50 dimensions.\n",
    "\n",
    "\n",
    "- Which we can further use for Sentiment Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Loading Model\n",
    "\n",
    "\n",
    "- We can save our model as a bin and review the content.\n",
    "\n",
    "\n",
    "- Further, we can also load the model using load() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LgSWGCQCeqW",
    "outputId": "cb0a8e37-0bc5-485c-bf05-8efc3779b371",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-30 16:33:03,240 : INFO : saving Word2VecKeyedVectors object under word2vector-50, separately None\n",
      "2018-11-30 16:33:03,242 : INFO : not storing attribute vectors_norm\n",
      "2018-11-30 16:33:03,512 : INFO : saved word2vector-50\n"
     ]
    }
   ],
   "source": [
    "#saving model\n",
    "model.wv.save('word2vector-50.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQPLNYwSCeqa"
   },
   "outputs": [],
   "source": [
    "#loading model\n",
    "model = gensim.models.Word2Vec.load('word2vec-50.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also load Google Word2Vec model by downloading file from here : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "\n",
    "- It is a pre-trained model by Google which was trained on Google News Data , where each word is represented by 300 embedding size.\n",
    "\n",
    "\n",
    "- There are many such pre-trained models available, few to name are GloVe (Global Vectors for Word Representation) by Stanford, FastText by Facebook AI Research Lab. One can load them and examine which works better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "\n",
    "- In this case study we learnt CBOW and Skip-gram in detail.\n",
    "\n",
    " \n",
    "- We also seen what obstacle we were facing and how researchers overcame it by adding Negative Sampling.\n",
    "\n",
    "\n",
    "- We trained the model by passing a clean tokenized corpora.\n",
    "\n",
    "\n",
    "- Finally, we looked at how we can Save / Load model and use it in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's next ?\n",
    "\n",
    "\n",
    "- We can use word embeddings as feature for Sentiment Analysis. How ? We'll see that in next Case study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "\n",
    "- Read about FastText here : https://fasttext.cc/\n",
    "\n",
    "\n",
    "- Read about GloVe here : https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "\n",
    "- Read about GoogleNews Word2Vec here : https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "\n",
    "- Wikipedia : https://en.wikipedia.org/wiki/Word2vec\n",
    "\n",
    "\n",
    "- **Mentions :** Images used here are imported from various other sources like ResearchGate, MiroMedium, RaRe Technologies and Others."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Learning - Word 2 vector - Gensim.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
